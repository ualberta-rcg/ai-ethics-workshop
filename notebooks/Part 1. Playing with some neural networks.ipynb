{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colourizing Photos\n",
    "This example takes a black and white photo and attempts to colorize that photo.  It is taken from a blog post called _[6 Deep Learning Applications a beginner can build in minutes (using Python)](https://www.analyticsvidhya.com/blog/2017/02/6-deep-learning-applications-beginner-python/)_.  It requires the use of an external tool which requires an account.  The account is free and it opens a lot of possibilities in the future.  The example is actually pretty neat to see too.\n",
    "\n",
    "Start by installing the Algorithmia library for Python by running the cell below.\n",
    "\n",
    "[Note that when using Google Colab you may recieve a warning for one or more packages that suggests that you restart your runtime environment.  This happens when a package was loaded automatically when the notebook first opened but was then updated with the install command.  Using the new version of the package requires a restart.  If this happens you should be able to simply click the `RESTART RUNTIME` button.  While this will erase any variables held in memory in relation to this notebook we do not have any variables held in memory at this time so this is not a problem and is a very good reason to load all you libraries at the start of the program.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install algorithmia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the algorithmia library installed we next do some setup on the Algorithmia website:\n",
    "\n",
    "1. **Register on [Algorithmia](https://algorithmia.com) and get your own API key.** You can find your API key in your profile once you have created an account.  Copy the API key and use it to replace the `...` in the cell below on the line that begins with `client =`.\n",
    "2. **Upload a black and white image.**  You can use [the included black and white jpegs] or grab your own with a quick Internet search.  Then on the Algorithmia site click on `Data Sources` on the left.  Then click on `My Hosted Data`.  Then click on the `New Collection` button.  Name the collection whatever you would like (I called mine \"colourizationTest\") and then drag and drop the image into the space.  Once uploaded there will be a little directory listing under the name of the file. Copy the location of the file by clicking the copy icon and use it to replace the `...` in the cell below on the line that begins with `input =` (It should look something _like_ `data://simpson/colouring/blackWhiteRainbow.jpeg`).\n",
    "3. **Run the code cell.**  This will result in a directory hierarchy that tells you where to find the output on the Algorithmia website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algorithmia\n",
    "\n",
    "client = Algorithmia.client('...') #replace ... with your API key\n",
    "algo = client.algo('deeplearning/ColorfulImageColorization/1.1.13')\n",
    "algo.set_options(timeout=300) # optional\n",
    "\n",
    "input = {\"image\": \"...\"}  #replace ... with path to your image\n",
    "\n",
    "print(algo.pipe(input).result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should receive output from this that will look like the following:\n",
    "\n",
    "`{'output': 'data://.algo/deeplearning/ColorfulImageColorization/temp/blackWhiteRainbow.png'}`\n",
    "\n",
    "In this output is the path for the image that the colouring algorithm created.  To see it go back to Algorithmia and click on `Data Sources` on the left hand menu, then `Algorithm Data`, then `ColorfulImageColorization` (the name of the algorithm we used), and finally on the `Temporary Files` tab to bring up the folder where the algorithm places it's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "Looking at an image and assigning probabilities that it belongs to each member of some set of categories is a use of deep learning that is growing in popularity.  Algorithmia has lots of options for this.  The one below works fairly well.  \n",
    "\n",
    "Assuming that you have already run the example above—and that Colab hasn't timed out—you can ignore the client API key now (it is commented out; if you need this option then remove the '#' at the front of the line starting with `client =` and then replace the `...` with your API key for Algorithmia).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algorithmia\n",
    "\n",
    "#client = Algorithmia.client('...')\n",
    "algo = client.algo('PetiteProgrammer/ImageClassifier/0.2.1')\n",
    "algo.set_options(timeout=300) # optional\n",
    "\n",
    "input = \"https://static.pexels.com/photos/126407/pexels-photo-126407.jpeg\"\n",
    "\n",
    "print(algo.pipe(input).result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like:\n",
    "\n",
    "    {'tags': [{'class': 'tiger_cat', 'confidence': 0.7151097059249878}, {'class': 'tabby', 'confidence': 0.16100214421749115}, {'class': 'lynx', 'confidence': 0.01275017112493515}, {'class': 'Egyptian_cat', 'confidence': 0.00386413955129683}, {'class': 'tiger', 'confidence': 0.0021544608753174543}, {'class': 'leopard', 'confidence': 0.0008957564132288098}, {'class': 'sock', 'confidence': 0.0007299092831090093}, {'class': 'washbasin', 'confidence': 0.0007146770949475467}, {'class': 'Persian_cat', 'confidence': 0.0007131726597435772}, {'class': 'plastic_bag', 'confidence': 0.00047915620962157845}]}\n",
    "\n",
    "Replace the `https://static.pexels.com/photos/126407/pexels-photo-126407.jpeg` in the cell above with the URL for other images and rerun the cell for each one to see what happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of other things that you can do with Algorithmia.  It is worth a bit of time to look at the demos page: https://demos.algorithmia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "So far all the work has been done on the Algorithmia server with algorithms that we are not seeing the details of (Algorithmia allows this, we just haven't taken advantage of it).  With this next example we'll take a little bit more ownership of what is going on to begin exposing a bit more of what is happening behind the scenes.\n",
    "\n",
    "In this example we'll use a trained model to generate text that _looks_ like Shakespeare.\n",
    "\n",
    "We'll start by installing the packages that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy tensorflow textgenrnn\n",
    "\n",
    "#It may be necessary it upgrade numpy.  \n",
    "#If you get an numpy related error then uncomment the line below and rerun this cell.\n",
    "#!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `numpy` is the Numerical Python library and it allows for some sophisticated matrix calculations to happen.  \n",
    "* `tensorflow` is a library that handles the actual functionality of the neural network.\n",
    "* `textgenrnn` is a library that runs a text generation training and production tool on top of tensorflow.  There is a substantial demo available as a Jupyter Notebook from GitHub for this library.  It may be found at https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb.  More details on textgenrnn with a different Shakespeare example than is being used here may be found at https://minimaxir.com/2018/05/text-neural-networks/.\n",
    "\n",
    "There is already a trained network trained to produce Shakespeare-like text and we'll begin by using this pretrained model to generate some sample lines (We'll get into what \"training\" a model means shortly).\n",
    "\n",
    "We begin by downloading the the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ualberta-rcg/ai-ethics-workshop/raw/master/notebooks/shakespeare_weights.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now available in the same directory as this notebook we can now call on it to produce some lines of Shakespeare-like text by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "import numpy\n",
    "textgen_write = textgenrnn('shakespeare_weights.hdf5')\n",
    "textgen_write.generate(3, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may see some warnings about depricated code the first time you run the code in the cell above.  This is Python telling us that some of the libraries that `textgenrnn` uses have better alternatives available.  For the moment these will not get in our way and we can safely ignore them for the time being.\n",
    "\n",
    "There are two variables that we can manipulate to modify the output.  In the last line of the cell above—`textgen_write.generate(3, temperature=0.5)`—there are two values separated by a comma within the parantheses.  The first (originally 3) is the number of lines to write.  Change this to any integer value and rerun the cell to get however many lines you would like.  The second—`temperature=0.5`—assigns a value (originally 0.5) to a variable called \"temperature\".  You can change this to any value from 0 to 1 to control how far the model should deviate from its initial inputs.  You should change this value to zero and rerun the model a few times and then change it to 1 and rerun the model a few times again to note the difference this value makes.\n",
    "\n",
    "We can also train our own models with textgenrnn, unlike the Algorithmia examples which all had pre-built models in the background.  This said, we won't actually be training our own models today.  Why?  It will simply take too long.  So... \n",
    "\n",
    "DO NOT RUN THE FOLLOWING CELL.   [In fact, we've made it impossible to run.  At least for the moment...]\n",
    "\n",
    "Instead, take the time to read the output immediately below the cell below.  It has been copied in from a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from textgenrnn import textgenrnn\n",
    "textgen_train = textgenrnn()\n",
    "textgen_train.train_from_file(file_path=\"shakespeare_complete-clean.txt\", num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    160,587 texts collected.\n",
    "    Training on 5,586,307 character sequences.\n",
    "    Epoch 1/5\n",
    "    43643/43643 [==============================] - 11092s 254ms/step - loss: 1.5619\n",
    "    ####################\n",
    "    Temperature: 0.2\n",
    "    ####################\n",
    "        The street of the world of the world, the great words\n",
    "\n",
    "        The state of the great words of the strange\n",
    "\n",
    "        The great will not see the world from the world\n",
    "\n",
    "    ####################\n",
    "    Temperature: 0.5\n",
    "    ####################\n",
    "        And the means of strange with such worms, they have the service of my princes had did\n",
    "\n",
    "        And with the princely cappiding the age?\n",
    "\n",
    "        The world to make the other and put on the stand\n",
    "\n",
    "    ####################\n",
    "    Temperature: 1.0\n",
    "    ####################\n",
    "      PANTHARD. O, my lord: I might. Haste war, if womer’s\n",
    "\n",
    "\n",
    "\n",
    "      SHARLEP. I have under his park out. That I am your\n",
    "\n",
    "    Epoch 2/5\n",
    "    43643/43643 [==============================] - 11129s 255ms/step - loss: 1.4340\n",
    "    ####################\n",
    "    Temperature: 0.2\n",
    "    ####################\n",
    "        That the world that hath a provoke of the forestance to the world and the brother hand and death and the world and the proceeding and the first and the servant\n",
    "\n",
    "        That he hath a part of the world to the fair fool,\n",
    "\n",
    "        That the world hath a prove and the world and the world,\n",
    "\n",
    "    ####################\n",
    "    Temperature: 0.5\n",
    "    ####################\n",
    "        Here is a speak to do a part of the state.\n",
    "\n",
    "        For now I see a promise and such a prove that the world,\n",
    "\n",
    "        For he says an adder ancient face.\n",
    "\n",
    "    ####################\n",
    "    Temperature: 1.0\n",
    "    ####################\n",
    "    By allto: ever try all the royal friend\n",
    "\n",
    "\n",
    "\n",
    "    It is a man’s totable there will be red\n",
    "\n",
    "    Epoch 3/5\n",
    "    43643/43643 [==============================] - 11326s 260ms/step - loss: 1.3844\n",
    "    ####################\n",
    "    Temperature: 0.2\n",
    "    ####################\n",
    "        That the state and the state of the state of the base particulation,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################\n",
    "    Temperature: 0.5\n",
    "    ####################\n",
    "\n",
    "\n",
    "        And he hath verited the care of the boots;\n",
    "\n",
    "        The valiant bear that can be received and bought\n",
    "\n",
    "    ####################\n",
    "    Temperature: 1.0\n",
    "    ####################\n",
    "      Thou hattst thou, ne, or ducat, grant thou surn.\n",
    "\n",
    "    To beg tell you fell in his fair.\n",
    "\n",
    "    TROILON.\n",
    "\n",
    "    Epoch 4/5\n",
    "    43643/43643 [==============================] - 12442s 285ms/step - loss: 1.3447\n",
    "    ####################\n",
    "    Temperature: 0.2\n",
    "    ####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################\n",
    "    Temperature: 0.5\n",
    "    ####################\n",
    "        The man the streets of the wars of the conceit.\n",
    "\n",
    "        And therefore there are a traitor and too\n",
    "\n",
    "        If the hand was to the fair pronounchest stores.\n",
    "\n",
    "    ####################\n",
    "    Temperature: 1.0\n",
    "    ####################\n",
    "    TITINIUS.\n",
    "\n",
    "        Attendants tonight them so. Morningers; lilie to seek\n",
    "\n",
    "        How time question sick unojUST; therefore should stir witness.\n",
    "\n",
    "    Epoch 5/5\n",
    "    43643/43643 [==============================] - 18955s 434ms/step - loss: 1.3090\n",
    "    ####################\n",
    "    Temperature: 0.2\n",
    "    ####################\n",
    "        The world is not the field of the heart of his son.\n",
    "\n",
    "        The seat will be the world to the beard.\n",
    "\n",
    "        The worst of the war with the season of the shame!\n",
    "\n",
    "    ####################\n",
    "    Temperature: 0.5\n",
    "    ####################\n",
    "          BENEDICK.\n",
    "\n",
    "    For the content of a little will the good tongue hath deceived him.\n",
    "\n",
    "        For your sun had slain to fear the world,\n",
    "\n",
    "    ####################\n",
    "    Temperature: 1.0\n",
    "    ####################\n",
    "      CLEOPATRA. The sibler tayance youth, be it of the ear\n",
    "\n",
    "\n",
    "\n",
    "    CONSTANCE.\n",
    "\n",
    "    CPU times: user 1d 17h 23min 42s, sys: 10h 24min 57s, total: 2d 3h 48min 39s\n",
    "    Wall time: 18h 12min 47s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the total time it took to just run 5 cycles through the data as listed in the last two lines of the output.  18h is a lot.  This was on a regular laptop.  It becomes easy to see that with a promised speedup of 10x-15x from Graphics Processing Units (GPUs) for this work why they quickly become a necessity.\n",
    "\n",
    "Ask yourself the following questions:\n",
    "\n",
    "1. What seems good about the output?\n",
    "2. What seems odd about the output?\n",
    "\n",
    "Once you've thought about this a bit the next step is to look at the input data and try to see if you can map what is working well to what isn't from the source.  You can see the source data at https://raw.githubusercontent.com/ualberta-rcg/ai-ethics-workshop/master/notebooks/shakespeare_complete-clean.txt.\n",
    "\n",
    "Once you've seen the source data you should ask yourself the following questions:\n",
    "\n",
    "1. What did the output above mean when it said, \"160,587 texts collected.\"?\n",
    "2. What formatting artifacts are affecting the output?\n",
    "3. Keeping in mind that this file has already been \"cleaned\" (you can see the original version at https://raw.githubusercontent.com/ualberta-rcg/ai-ethics-workshop/master/notebooks/shakespeare_complete-original.txt) how would you further modify the input to improve the results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[If you really, really want to run the code cell above for yourself then run the cell below to download the source file and then run the cell above.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ualberta-rcg/ai-ethics-workshop/master/notebooks/shakespeare_complete-clean.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
